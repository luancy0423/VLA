import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
from timm.models.vision_transformer import VisionTransformer

# ğŸŒŸ è¯­ä¹‰åˆ†è§£æ¨¡å—æ ¸å¿ƒç»„ä»¶
class SemanticDecomposition(nn.Module):
    def __init__(self, visual_dim=1280, text_dim=4096):
        super().__init__()
        
        # å±æ€§è§£æåˆ†æ”¯
        self.attr_net = nn.Sequential(
            nn.Conv2d(visual_dim, 256, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            ResNet18Block(256),
            MultiHeadAttention(256, 4)  # å¤šå¤´æ³¨æ„åŠ›
        )
        
        # å…³ç³»æ¨ç†åˆ†æ”¯
        self.relation_net = GraphAttentionNetwork(
            node_dim=256,
            edge_dim=64,
            num_heads=4
        )
        
        # ä»»åŠ¡å¯¹é½äº¤å‰æ³¨æ„åŠ›
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=512,
            num_heads=8,
            kdim=text_dim,
            vdim=text_dim
        )

    def forward(self, visual_feat, text_feat):
        # å±æ€§ç‰¹å¾æå–
        attr_feat = self.attr_net(visual_feat)  # [B,256,H',W']
        
        # æ„å»ºåœºæ™¯å›¾
        node_features = self.graph_builder(visual_feat)
        adj_matrix = self.relation_net(node_features)
        
        # è·¨æ¨¡æ€å¯¹é½
        aligned_feat, _ = self.cross_attn(
            query=attr_feat.flatten(2),
            key=text_feat,
            value=text_feat
        )
        return aligned_feat, adj_matrix

# ğŸŒŸ æ”¹è¿›çš„åŠ¨ä½œç”Ÿæˆå±‚
class ActionDecoder(nn.Module):
    def __init__(self, hidden_dim=2048):
        super().__init__()
        self.bin_discretizer = nn.Embedding(256, 6)  # 6DåŠ¨ä½œç©ºé—´
        
        self.lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=1024,
            num_layers=3,
            bidirectional=True
        )
        
        # CoTè§„åˆ’å¤´
        self.cot_head = nn.Sequential(
            nn.Linear(2048, 4096),
            nn.GELU(),
            nn.Linear(4096, 4096)  # åŒ¹é…Yi-34Béšè—å±‚
        )
        
        # åŠ¨ä½œé¢„æµ‹å¤´
        self.action_head = nn.Sequential(
            nn.Linear(2048, 512),
            nn.ReLU(),
            nn.Linear(512, 256 * 7)  # 7ä¸ªåŠ¨ä½œç»´åº¦Ã—256 bins
        )

    def forward(self, fused_feat):
        # ç”ŸæˆCoTè§„åˆ’æ–‡æœ¬
        cot_logits = self.cot_head(fused_feat)
        
        # æ—¶åºåŠ¨ä½œç”Ÿæˆ
        lstm_out, _ = self.lstm(fused_feat.unsqueeze(1))
        action_logits = self.action_head(lstm_out)
        return cot_logits, action_logits.view(-1,7,256)

# ğŸŒŸ å®Œæ•´æ¨¡å‹æ•´åˆ
class UniversalGraspingVLA(nn.Module):
    def __init__(self):
        super().__init__()
        
        # è§†è§‰ç¼–ç å™¨
        self.visual_encoder = VisionTransformer(
            img_size=448,
            patch_size=14,
            embed_dim=1280,
            depth=32,
            num_heads=16
        )
        
        # è¯­è¨€æ¨¡å‹ï¼ˆä½¿ç”¨HuggingFaceæ¥å£ï¼‰
        self.text_encoder = AutoModel.from_pretrained("01-ai/Yi-VL-34B")
        self.tokenizer = AutoTokenizer.from_pretrained("01-ai/Yi-VL-34B")
        
        # è¯­ä¹‰åˆ†è§£æ¨¡å—
        self.sem_decomp = SemanticDecomposition()
        
        # åŠ¨æ€æŠ•å½±å±‚
        self.projector = nn.Linear(1280+4096, 2048)
        
        # åŠ¨ä½œè§£ç å™¨
        self.action_decoder = ActionDecoder()
        
        # ğŸŒŸ åŠ¨æ€è¯æ±‡è¡¨çº¦æŸ
        self.register_buffer('action_vocab', 
            torch.randint(0, 256, (7, 256)))

    def forward(self, images, instructions):
        # è§†è§‰ç‰¹å¾æå–
        visual_feat = self.visual_encoder(images)  # [B, 197, 1280]
        patch_feat = visual_feat[:, 1:]  # å»é™¤cls token
        
        # æ–‡æœ¬ç‰¹å¾æå–
        text_input = self.tokenizer(
            instructions, 
            return_tensors='pt',
            padding=True
        ).to(images.device)
        text_feat = self.text_encoder(**text_input).last_hidden_state
        
        # è¯­ä¹‰åˆ†è§£ä¸å¯¹é½
        aligned_feat, adj_matrix = self.sem_decomp(
            patch_feat.permute(0,2,1).view(-1,1280,16,16),
            text_feat
        )
        
        # ç‰¹å¾èåˆ
        fused_feat = self.projector(
            torch.cat([visual_feat[:,0], text_feat[:,0]], dim=1)
        )
        
        # åŠ¨ä½œç”Ÿæˆ
        cot_logits, action_logits = self.action_decoder(fused_feat)
        
        # ğŸŒŸ åº”ç”¨åŠ¨æ€è¯æ±‡è¡¨çº¦æŸ
        action_logits = action_logits * self.action_vocab.unsqueeze(0)
        return cot_logits, action_logits

# ğŸŒŸ è®­ç»ƒä¼˜åŒ–ç­–ç•¥
class CustomTrainer:
    def __init__(self, model, stages=3):
        self.model = model
        self.stages = stages
        
    def train_stage(self, stage, dataloader):
        # åˆ†é˜¶æ®µè§£å†»å‚æ•°
        if stage == 1:
            freeze_params(self.model.text_encoder)
            freeze_params(self.model.action_decoder)
        elif stage == 2:
            unfreeze_params(self.model.sem_decomp.relation_net)
        elif stage == 3:
            unfreeze_params(self.model)
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        scaler = torch.cuda.amp.GradScaler()
        
        for batch in dataloader:
            with torch.cuda.amp.autocast():
                cot_pred, action_pred = self.model(batch['images'], batch['instructions'])
                loss = self.calculate_loss(cot_pred, action_pred, batch)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
    def calculate_loss(self, cot_pred, action_pred, batch):
        # CoTæ–‡æœ¬æŸå¤±
        cot_loss = F.cross_entropy(
            cot_pred.view(-1, 32000),  # Yi-34Bè¯æ±‡è¡¨å¤§å°
            batch['cot_labels'].view(-1)
        )
        
        # åŠ¨ä½œç¦»æ•£åŒ–æŸå¤±
        action_loss = sum(
            F.cross_entropy(
                action_pred[:,i], 
                batch['action_labels'][:,i]
            ) for i in range(7)
        )
        
        # ğŸŒŸ å¯¹æŠ—è®­ç»ƒæŸå¤±
        real_actions = batch['action_labels']
        fake_actions = action_pred.argmax(-1)
        adv_loss = F.mse_loss(
            self.discriminator(real_actions),
            self.discriminator(fake_actions.detach())
        )
        
        return cot_loss + 0.7*action_loss + 0.3*adv_loss

# è¾…åŠ©å‡½æ•°
def freeze_params(module):
    for param in module.parameters():
        param.requires_grad_(False)

def unfreeze_params(module):
    for param in module.parameters():
        param.requires_grad_(True)
